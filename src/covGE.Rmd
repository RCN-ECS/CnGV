---
title: "co-counter gradient variation"
author: "Katie Lotterhos"
date: "5/30/2019"
output: html_document
---
```{r}
library("parallel")
library("doParallel")
library("tidyverse")
```

### Simulations ###
```{r}
## Co-gradient

G <- c(-1, 1) # G is vector of y-values
E <- c(1, -1) # E is vector of x-values
(covGE <- cov(G, E))
GxE = c(0, 0, 0, 0) #N1E1, N1E2, N2E1, N2E2

# countergradient
#G <- c(1, -1)
#E <- c(-1, 1)
#cov(G,E)
```

```{r Function to create simulated data}
### This function creates reciprocal transplant data ###
### returns the data, true cov(G,E), obs cov(G,E) ###

simulateGV <- function(G_diff_N1_N2, E_diff_N1_N2, n, GxE=c(0,0,0,0)){
  ## G_diff_N1_N2 genetic difference in phenotype from population N1 to N2
  ## E_diff_N1_N2 environment difference in phenotype from population N1 to N2
  ## n is the sample size per population-environment combo

  (G <- c(-G_diff_N1_N2/2, G_diff_N1_N2/2))
  (E <- c(-E_diff_N1_N2/2, E_diff_N1_N2/2))
  (covGE_true <- cov(G, E))
  #(corGE_true <- cor(G, E))

  P1_E1 <- G[1] + E[1] + GxE[1] + rnorm(n,0, 1)
  P1_E2 <- G[1] + E[2] + GxE[2] + rnorm(n,0, 1)
  P2_E1 <- G[2] + E[1] + GxE[3] + rnorm(n,0, 1)
  P2_E2 <- G[2] + E[2] + GxE[4] + rnorm(n,0, 1)

  phen <- c(P1_E1, P1_E2, P2_E1, P2_E2)
  gen <- rep(c("G1","G2"), each=n*2)
  env <- rep(c("E1", "E2", "E1", "E2"), each=n)

  data_GE <- data.frame(phen, gen, env)

  (covGE_data <- cov(c(mean(phen[gen=="G1"]),mean(phen[gen=="G2"])), 
    c(mean(phen[env=="E1"]), mean(phen[env=="E2"]))
    ))
  #(corGE_data <- cor(c(mean(phen[gen=="G1"]),mean(phen[gen=="G2"])), 
  #  c(mean(phen[env=="E1"]), mean(phen[env=="E2"]))
  #  ))
    
  return(list(data_GE=data_GE, 
              covGE_true=covGE_true, covGE_data=covGE_data)) 
              #corGE_true = corGE_true, corGE_data=corGE_data))
}
```

```{r Bootstrap Function for simulated data}

## Bootstrap to estimate error

bootstrapGV <- function(data_GE){

  G_means <- NULL
  E_means <- NULL
  for (j in 1:nlevels(data_GE$gen)){
    cond_G <- data_GE$gen==levels(data_GE$gen)[j]
    G_means[j] <- mean(sample(data_GE$phen[cond_G], size=sum(cond_G), replace=TRUE))
    
    cond_E <- data_GE$env==levels(data_GE$env)[j]
    E_means[j] <- mean(sample(data_GE$phen[cond_E], size=sum(cond_E), replace=TRUE))
  }
  covGE_boot <- cov(G_means, E_means)
  #corGE_boot <- cor(G_means, E_means)
  return(list(covGE_boot))#, corGE_boot))
}


```

```{r GetPower Function}

## Function to calculate power based on output from below for-loop

GetPower <- function(x){

  powerdat = x %>%
  group_by(samplesize,GE_Diff) %>%
  summarise(ct = length(overunder),
            cat = sum(overunder))
  powerdat$power = (powerdat$cat/powerdat$ct)

  return(as.data.frame(powerdat))
}

```
### Power Analysis Simulations ###
```{r Simulation}
setwd("~/Documents/GitHub/CnGV/src/")

#cores=detectCores()
#cl <- makeCluster(cores[1]-1) #not to overload your computer
#registerDoParallel(cl)

nrep = seq(from = 1, to = 5, by = 1)
samplesize = seq(from = 1, to = 10, by = 1) # Must be Integer
diff_effect = seq(from = 1, to = 5, by = 1) 
#row = 0 # for indexing
#nsims = 7 # Multiply against rep for total number sims

## Parallelize

#foreach(s = 1:nsims, .combine = rbind) %dopar% { 
  
## Establish datasets for output  

simout = data.frame()
power_data = data.frame()

## Simulation 

for(j in 1:length(nrep)){ 
  for(k in 1:length(samplesize)) {
    for(l in 1:length(diff_effect)) {
      
       # Set parameters 
       rep = nrep[j]
       val = samplesize[k] 
       effect_val = diff_effect[l]
      
       cat(j,k,l, "\n")  # Counter 
       
       # Run functions
       simout_temp <- simulateGV(effect_val, effect_val, val) #Run function to generate data 
       sampling_dist_cov <- replicate(1000, bootstrapGV(simout_temp$data_GE)[1], simplify=TRUE) #Bootstrap
       CI = quantile(unlist(sampling_dist_cov), probs=c(0.025, 0.975), type=1) #Get confidence intervals
       
       overunder = 0
       if (CI[1] <= 0) {overunder = 0} else 
         if (CI[2] <= 0) {overunder = 0} else 
           overunder = 1

       # Data output  
       simoutk = data.frame("replicate" = rep,
                       "samplesize" = val,
                       "GE_Diff" = effect_val,
                       "covGE_true" = simout_temp$covGE_true,
                       "covGE_data" = simout_temp$covGE_data, 
                       "lwrCI" = CI[1],
                       "uprCI" = CI[2],
                       "overunder" = overunder)
        simout = rbind(simout,simoutk)
        
    }
  }
  power_data = GetPower(simout)
 # write.csv(simout,paste("SimResults_06122019",s,".csv",sep="_"),row.names=FALSE)
}
  
## To stop:
#stopCluster(cl)
```
*Colate Data from Sims*

```{r}
# Use below only running in parallel. 

## List all the files

temp <- list.files(pattern= "*06102019")
temp1 = do.call(rbind, lapply(temp, function(x) read.csv(x, stringsAsFactors = FALSE)))

## Add column for replicate

nrep = (length(diff_effect)*length(samplesize))
totsim = (nsims*length(rep))
temp1$replicate = rep(1:totsim, each = nrep)  

```

*Power Heatmap*

Power is the proportion of replicates whose 95% CI do not include 0

```{r}

## Plot Heatmap

simplot = ggplot(data = power_data, aes(x = samplesize, y = GE_Diff))+
  geom_tile(aes(fill = power), colour = "white") + scale_fill_gradient(low = "white", high = "steelblue")+
  theme_bw()+ 
  ylab("Difference in G and E") + xlab("Sample Size")+
  theme_bw(base_size = 30, base_family = "Helvetica")+ 
  theme(strip.background =element_rect(fill="white"))+
  theme(strip.text = element_text(colour = 'black')) +
  theme(plot.background = element_blank(),panel.grid.major = element_blank(),panel.grid.minor = element_blank())+
  theme(axis.ticks = element_line(colour = "black"))+
  theme(axis.text= element_text(colour = "black"))+
  labs(colour = "Sample Size")+
  theme(legend.position="bottom")+
  theme(legend.title = element_text(size = 14),
          legend.text = element_text(size = 14))
  #theme(legend.position="none")
simplot

```
### Meta-Analysis Data Analysis ###
Code for Real Data: 

For population level, need function to establish G_means and E_means. 
1. This info should be in factor form. 
2. May need to translate it into quantitative form?

For gradient level, need a bit more: 
1. Estimate G_means
To calculate G_means, generate slope of lines for each G across all E's (lm(phen~env)). G_means = avg phenotype for each genotype at E_hat.

2. Estimate E_means
To calculate E_means, use same slope of lines as G_means. E_means = average phenotype across genotypes at each genotype's environment.

In all cases, need to standardize phenotype before estimation.
Standardize = (Data - avg(data))/std dev(data)

For the falc_dat, we don't know which genotype comes from which environment. 
I'm going to arbitrarily assign genotypes to continuous environments just to get the code working.

NOTE: if studies do not provide data about the environmental conditions that genotypes originated in, we cannot use for analyses.

Population Assignments = (this the order presented in the data, may seem confusing)
pop B = gen 0.5
pop A = gen 0.8
pop D = gen 1.1
pop C = gen 1.4

Co-Gradient Scenario: 
A originates in env [90-99]
B originates in env [100-109]
C originates in env [110-119]
D originates in env [120-129]

Counter-Gradient Scenario: 
A originates in env [120-129]
B originates in env [110-119]
C originates in env [100-109]
D originates in env [90-99]


```{r}

## Gradient level analyses
Gradient_level_fun  <- function(z){
  
  ## Standardize data
  dat_avg = mean(z$phen) 
  dat_std = sd(z$phen)
  z$phen_corrected = ((z$phen-dat_avg)/dat_std)

  Cov_matrix = data.frame() # Covariance matrix
  predicted_data = data.frame() # Model output 
    
  # Environmental parameters
  E_means = c(E1_mean,E2_mean,E3_mean,E4_mean) # Needs to be generalized.
  E_hat = as.integer(mean(z$env))
  
  # Model
  grand_mod = lm(phen_corrected ~ env + gen, data = z) 

  # Predict phenotypic value for each genotype across <all> environments
  temp_dat <- expand.grid(gen = unique(z$gen),
                                env = seq(from = as.integer(min(z$env)),to = as.integer(max(z$env)),by=1),
                                phen_corrected=0)
  mm = model.matrix(terms(grand_mod),temp_dat)
  temp_dat$phen_corrected = mm %*% coef(grand_mod) # Predicted phenotypes

  # Add Error
  se <- sqrt(diag(vcov(grand_mod)))
  se <- data.frame(se)
  setDT(se, keep.rownames = TRUE)
  se <- filter(se, rn != "env")
  se$gen = levels(z$gen)
  
  predicted_data = left_join(temp_dat, se, by = "gen")
  predicted_data$G_upr = predicted_data$phen_corrected + predicted_data$se
  predicted_data$G_lwr = predicted_data$phen_corrected - predicted_data$se

  # G_means
  G_means = filter(predicted_data, env == E_hat)
  
  # E_means
  E_means1 = data.frame()
  for(j in 1:length(E_means)){
    env_dat = E_means[j]
    E_dat = filter(predicted_data, env == env_dat)
    E_temp_mean = mean(E_dat$phen_corrected)
    E_upr = E_temp_mean + (sigma(grand_mod))
    E_lwr = E_temp_mean - (sigma(grand_mod))
    E_tempdat = data.frame()
    E_tempdat = data.frame("E_means" = E_temp_mean,
                           "E_upr" = E_upr,
                           "E_lwr" = E_lwr)
    E_means1 = rbind(E_means1, E_tempdat)
  }
  
  # Data output
  Cov_matrix = data.frame("gen" = unique(z$gen),
                          "env" = E_means,
                          "G_means" = G_means$phen_corrected,
                          "G_upr" = G_means$G_upr,
                          "G_lwr" = G_means$G_lwr,
                          "E_means" = E_means1$E_means,
                          "E_upr" = E_means1$E_upr,
                          "E_lwr" = E_means1$E_lwr,
                          "g_se" = se$se,
                          "e_se" = rep((sigma(grand_mod)),nrow(E_means1)),
                          "e_sampsize" = rep(nrow(z),nrow(E_means1)),
                          "g_sampsize" = rep(nrow(z)/length(unique(z$gen)),nrow(E_means1)))
  
  return(as.data.frame(Cov_matrix))
}

```


```{r}

## Load data

falc_dat = read.csv("~/Documents/GitHub/CnGV/data/Data_p137_Falconer1981.csv")
colnames(falc_dat) = c("env", "phen", "gen")
falc_dat$gen = as.factor(falc_dat$gen)

## Specific to falconer data...

  E1 = falc_dat[falc_dat$env > 90 & falc_dat$env < 99.99, ]
  E1_mean = as.integer(mean(E1$env))
  E2 = falc_dat[falc_dat$env > 100 & falc_dat$env < 109.99, ]
  E2_mean = as.integer(mean(E2$env))
  E3 = falc_dat[falc_dat$env > 110 & falc_dat$env < 119.99, ]
  E3_mean = as.integer(mean(E3$env))
  E4 = falc_dat[falc_dat$env > 120 & falc_dat$env < 129.99, ]
  E4_mean = as.integer(mean(E4$env))
  
## Generate predicted phenotypes for each G and E
covariance_data = Gradient_level_fun(falc_dat)


## Covariance from data
G <- c(covariance_data$G_means) 
E <- c(covariance_data$E_means) 
(covGE <- cov(G, E)) # For Falconer data, actual covariance = 0.51

```

```{r}
m = covariance_data

data_generation <- function(m){ # Insert covariance data generated above
  
  temp_dat = data.frame()
  
  for(i in 1:nrow(m)){
    
    # Generate G's and E's based on means and standard error from model fits
    G <- rnorm(100,m$G_means[i],(m$g_se[i]*sqrt(m$g_sampsize[i])))
    E <- rnorm(100,m$E_means[i],(m$e_se[i]*sqrt(m$e_sampsize[i])))
    
    temp_dat1 = data.frame("G" = G,
                           "E" = E, 
                           "gen" = rep(m$gen[i],100),
                           "env" = rep(m$env[i],100))
    temp_dat = rbind(temp_dat,temp_dat1)
  }
    return(temp_dat)
}
```

```{r}

data_distribution <- data_generation(m = covariance_data)
    
cov_mat_fun <- function(temp_dat){ 
  
  temp_dat2 = split(temp_dat,temp_dat$gen)
  
  temp_covmat = data.frame()
  
  for(j in 1:length(temp_dat2)){
        G_temp = sample(temp_dat2[[j]][,1], size = 1, replace=TRUE)
        E_temp = sample(temp_dat2[[j]][,2], size = 1, replace=TRUE)
        temp_covmat. = data.frame("G_temp" = G_temp,
                                  "E_temp" = E_temp,
                                  "gen_ID" = temp_dat2[[j]][1,3],
                                  "env_ID" = temp_dat2[[j]][1,4])
        temp_covmat = rbind(temp_covmat,temp_covmat.)
      }
      
  cov_est = cov(temp_covmat$G_temp,temp_covmat$E_temp)
  return(cov_est)
} 

cov_dat <-  replicate(1000, cov_mat_fun(data_distribution), simplify=TRUE) # Dataset with 1000 predicted covariances
cov_avg = mean(cov_dat)  # should be similar to original average 
cov_CI = quantile(unlist(cov_dat), probs=c(0.025, 0.975), type=1) # Get confidence intervals around covariance

```

*Extra code*
May want to use in the future. We had correlation, but I (MA) removed it from current code since we are only using 2 estimates for G and E and therefore cannot estimate a correlation. 
```{r Extra Code}
## Was a part of code used in for-loop above, but presently unused. 
rownames(sampling_dist_cov) <- c("covGE_boot", "corGE_boot")
sampling_dist_cov <- data.frame(as.matrix(unlist(t(sampling_dist_cov))))

# cor
quantile(unlist(sampling_dist_cov[2,]), probs=c(0.025, 0.975), type=1)
# this is always 1 or -1 because we only have 2 data points

boxplot(phen~gen)

var(phen)

```
